# import requests
# from bs4 import BeautifulSoup
# import urllib.parse as urlparse

# def request(url):
#     try:
#         return requests.get(url)
#     except requests.exception.ConnectionError:
#         pass

# target_url = "http://10.0.2.4/mutillidae/index.php?page=dns-lookup.php"
# response = request(target_url)

# parsed_html = BeautifulSoup(response.content, 'lxml')
# forms_list = parsed_html.findAll("form")

# for form in forms_list:
#     action = form.get("action")
#     post_url = urlparse.urljoin(target_url, action)
#     method = form.get("method")

#     input_list = form.findAll("input")
#     post_data_dict = {}
#     for input in input_list:
#         input_name = input.get("name")
#         input_type = input.get("type")
#         input_value = input.get("value")
#         if input_type == "text":
#             input_value = "test"

#         post_data_dict[input_name] = input_value

#     result = requests.post(post_url , data=post_data_dict)
#     print(result.content)                


import re
import requests
import urllib.parse

class Scanner:
    def __init__(self, url):
        self.target_url = url
        self.web_page_links = []

    @staticmethod
    def extract_link(url):
        response = requests.get(url)  # To get all the links on a webpage
        return re.findall('(?:href=")(.*?)"', response.content.decode(errors="ignore"))

    def crawler(self, url= None):
        if url == None:
            url = self.target_url
        href_link = self.extract_link(url)
        for link in href_link:
            link = urllib.parse.urljoin(url, link)

            if '#' in link:
                link = link.split('#')[0]

            if self.target_url in link and link not in self.webpage_links:
                self.webpage_links.append(link)
                print(link)
                self.crawler(link)    
